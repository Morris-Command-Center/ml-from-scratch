{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent From Scratch\n",
    "\n",
    "The optimization algorithm that powers all of deep learning.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How gradient descent finds the minimum of a function\n",
    "- What gradients are and how to compute them\n",
    "- The effect of learning rate on convergence\n",
    "- How to apply gradient descent to linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Intuition: Rolling Downhill\n",
    "\n",
    "Imagine you're blindfolded on a hilly landscape and want to find the lowest point:\n",
    "- Feel the slope under your feet\n",
    "- Take a step downhill\n",
    "- Repeat until you stop going down\n",
    "\n",
    "That's gradient descent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with a simple 1D function: f(x) = x^2\n",
    "# The minimum is obviously at x=0, but let's find it with gradient descent\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"Our simple function\"\"\"\n",
    "    return x ** 2\n",
    "\n",
    "def df(x):\n",
    "    \"\"\"The derivative (gradient) of f: df/dx = 2x\"\"\"\n",
    "    return 2 * x\n",
    "\n",
    "# Visualize\n",
    "x_range = np.linspace(-5, 5, 100)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_range, f(x_range), 'b-', linewidth=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x) = x²')\n",
    "plt.title('Our Simple Function')\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Gradient Descent Algorithm\n",
    "\n",
    "```\n",
    "x_new = x_old - learning_rate * gradient\n",
    "```\n",
    "\n",
    "- **gradient**: The slope at current position (tells us which way is uphill)\n",
    "- **learning_rate**: How big a step we take\n",
    "- **minus sign**: We go *opposite* to the gradient (downhill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_1d(f, df, x_start, learning_rate, n_steps):\n",
    "    \"\"\"\n",
    "    Run gradient descent on a 1D function.\n",
    "    \n",
    "    Returns history of x values and function values.\n",
    "    \"\"\"\n",
    "    x = x_start\n",
    "    history = [(x, f(x))]\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        gradient = df(x)\n",
    "        x = x - learning_rate * gradient\n",
    "        history.append((x, f(x)))\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Run gradient descent starting from x=4\n",
    "history = gradient_descent_1d(f, df, x_start=4, learning_rate=0.1, n_steps=20)\n",
    "\n",
    "# Plot the path\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_range, f(x_range), 'b-', linewidth=2, label='f(x) = x²')\n",
    "\n",
    "# Plot the steps\n",
    "xs, ys = zip(*history)\n",
    "plt.plot(xs, ys, 'ro-', markersize=8, label='Gradient descent path')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Gradient Descent Finding the Minimum')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Started at x={history[0][0]:.4f}, f(x)={history[0][1]:.4f}\")\n",
    "print(f\"Ended at x={history[-1][0]:.4f}, f(x)={history[-1][1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Learning Rate Matters!\n",
    "\n",
    "Let's see what happens with different learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "learning_rates = [0.01, 0.1, 0.9]\n",
    "titles = ['Too Small (0.01)', 'Just Right (0.1)', 'Too Large (0.9)']\n",
    "\n",
    "for ax, lr, title in zip(axes, learning_rates, titles):\n",
    "    history = gradient_descent_1d(f, df, x_start=4, learning_rate=lr, n_steps=20)\n",
    "    xs, ys = zip(*history)\n",
    "    \n",
    "    ax.plot(x_range, f(x_range), 'b-', linewidth=2)\n",
    "    ax.plot(xs, ys, 'ro-', markersize=6)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('f(x)')\n",
    "    ax.set_title(f'{title}\\nFinal x={xs[-1]:.4f}')\n",
    "    ax.set_ylim(-1, 20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Applying to Linear Regression\n",
    "\n",
    "Now let's use gradient descent to fit a linear regression, instead of the closed-form solution.\n",
    "\n",
    "For MSE loss with linear regression:\n",
    "- ∂MSE/∂m = -(2/n) * Σ(y - ŷ) * x\n",
    "- ∂MSE/∂b = -(2/n) * Σ(y - ŷ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the same data as the linear regression notebook\n",
    "np.random.seed(42)\n",
    "X = np.random.uniform(0, 10, 50)\n",
    "true_slope, true_intercept = 2, 1\n",
    "noise = np.random.normal(0, 2, 50)\n",
    "y = true_slope * X + true_intercept + noise\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.7)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Our Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(X, y, m, b):\n",
    "    \"\"\"\n",
    "    Compute gradients of MSE with respect to m and b.\n",
    "    \n",
    "    These are the partial derivatives of the loss function.\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    y_pred = m * X + b\n",
    "    error = y - y_pred\n",
    "    \n",
    "    # Gradients\n",
    "    dm = -(2/n) * np.sum(error * X)\n",
    "    db = -(2/n) * np.sum(error)\n",
    "    \n",
    "    return dm, db\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def gradient_descent_linear(X, y, learning_rate=0.01, n_steps=1000):\n",
    "    \"\"\"\n",
    "    Fit linear regression using gradient descent.\n",
    "    \"\"\"\n",
    "    # Initialize with random values\n",
    "    m, b = 0.0, 0.0\n",
    "    \n",
    "    history = []\n",
    "    \n",
    "    for step in range(n_steps):\n",
    "        # Calculate gradients\n",
    "        dm, db = compute_gradients(X, y, m, b)\n",
    "        \n",
    "        # Update parameters\n",
    "        m = m - learning_rate * dm\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        # Track progress\n",
    "        y_pred = m * X + b\n",
    "        loss = mse(y, y_pred)\n",
    "        history.append({'step': step, 'm': m, 'b': b, 'loss': loss})\n",
    "    \n",
    "    return m, b, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run gradient descent\n",
    "m_gd, b_gd, history = gradient_descent_linear(X, y, learning_rate=0.01, n_steps=1000)\n",
    "\n",
    "print(f\"Gradient Descent: m={m_gd:.4f}, b={b_gd:.4f}\")\n",
    "print(f\"True values:      m={true_slope}, b={true_intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss over time\n",
    "losses = [h['loss'] for h in history]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Loss Decreasing Over Training')\n",
    "plt.yscale('log')  # Log scale to see the detail\n",
    "plt.show()\n",
    "\n",
    "print(f\"Initial loss: {losses[0]:.4f}\")\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the parameter trajectory\n",
    "ms = [h['m'] for h in history]\n",
    "bs = [h['b'] for h in history]\n",
    "\n",
    "# Create loss surface\n",
    "m_range = np.linspace(-1, 4, 100)\n",
    "b_range = np.linspace(-3, 5, 100)\n",
    "M, B = np.meshgrid(m_range, b_range)\n",
    "\n",
    "MSE_grid = np.zeros_like(M)\n",
    "for i in range(M.shape[0]):\n",
    "    for j in range(M.shape[1]):\n",
    "        y_pred = M[i,j] * X + B[i,j]\n",
    "        MSE_grid[i,j] = mse(y, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "contour = plt.contour(M, B, MSE_grid, levels=30)\n",
    "plt.plot(ms, bs, 'r.-', markersize=2, linewidth=1, label='GD path')\n",
    "plt.plot(ms[0], bs[0], 'go', markersize=10, label='Start')\n",
    "plt.plot(ms[-1], bs[-1], 'r*', markersize=15, label='End')\n",
    "plt.plot(true_slope, true_intercept, 'b^', markersize=12, label='True')\n",
    "plt.xlabel('Slope (m)')\n",
    "plt.ylabel('Intercept (b)')\n",
    "plt.title('Gradient Descent Path on Loss Surface')\n",
    "plt.legend()\n",
    "plt.colorbar(contour, label='MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization: our fitted line\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.7, label='Data')\n",
    "plt.plot([0, 10], [b_gd, 10*m_gd + b_gd], 'r-', linewidth=2, \n",
    "         label=f'GD: y = {m_gd:.2f}x + {b_gd:.2f}')\n",
    "plt.plot([0, 10], [true_intercept, 10*true_slope + true_intercept], 'g--', \n",
    "         linewidth=2, alpha=0.5, label=f'True: y = {true_slope}x + {true_intercept}')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression via Gradient Descent')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment: Learning Rate Effects\n",
    "\n",
    "Try changing the learning rate and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "learning_rates = [0.001, 0.01, 0.05, 0.1]\n",
    "\n",
    "for ax, lr in zip(axes.flat, learning_rates):\n",
    "    _, _, hist = gradient_descent_linear(X, y, learning_rate=lr, n_steps=500)\n",
    "    losses = [h['loss'] for h in hist]\n",
    "    \n",
    "    ax.plot(losses)\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('MSE')\n",
    "    ax.set_title(f'Learning Rate = {lr}\\nFinal Loss = {losses[-1]:.4f}')\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Gradient descent iteratively finds the minimum** by taking steps opposite to the gradient\n",
    "2. **The learning rate is crucial** - too small is slow, too large diverges\n",
    "3. **The gradient tells us the direction of steepest increase**, so we go the opposite way\n",
    "4. **This is how neural networks learn** - same principle, more parameters\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "- **Stochastic Gradient Descent (SGD)**: Use random subsets for efficiency\n",
    "- **Momentum**: Build up speed in consistent directions\n",
    "- **Adam**: Adaptive learning rates per parameter\n",
    "- **Apply to neural networks**: Same idea, thousands of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=\" * 50)\n",
    "print(\"GRADIENT DESCENT SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Learning rate: 0.01\")\n",
    "print(f\"Steps: 1000\")\n",
    "print(f\"Final parameters: m={m_gd:.4f}, b={b_gd:.4f}\")\n",
    "print(f\"True parameters: m={true_slope}, b={true_intercept}\")\n",
    "print(f\"Final MSE: {losses[-1]:.4f}\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
