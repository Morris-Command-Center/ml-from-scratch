{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression From Scratch\n",
    "\n",
    "The simplest ML model: fit a line through data points.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to implement linear regression using only numpy\n",
    "- The math behind finding the best-fit line\n",
    "- What a loss function is and why it matters\n",
    "- How to visualize your model's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make plots look nice\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Some Data\n",
    "\n",
    "Let's create some fake data that roughly follows a line, with some noise added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 50 random x values between 0 and 10\n",
    "X = np.random.uniform(0, 10, 50)\n",
    "\n",
    "# The \"true\" relationship: y = 2x + 1 (plus some noise)\n",
    "true_slope = 2\n",
    "true_intercept = 1\n",
    "noise = np.random.normal(0, 2, 50)  # Gaussian noise\n",
    "\n",
    "y = true_slope * X + true_intercept + noise\n",
    "\n",
    "# Plot it\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.7, label='Data points')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Our Dataset (true line: y = 2x + 1)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"We have {len(X)} data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Goal\n",
    "\n",
    "We want to find the line `y = mx + b` that best fits our data.\n",
    "\n",
    "- **m** is the slope (how steep the line is)\n",
    "- **b** is the intercept (where the line crosses the y-axis)\n",
    "\n",
    "But what does \"best\" mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Loss Function: Mean Squared Error\n",
    "\n",
    "We need a way to measure how \"wrong\" our line is. The most common way is **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "Where:\n",
    "- $y_i$ is the actual value\n",
    "- $\\hat{y}_i$ is our prediction\n",
    "- We square the errors so negatives don't cancel out positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, m, b):\n",
    "    \"\"\"Make predictions using y = mx + b\"\"\"\n",
    "    return m * X + b\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"Calculate the mean squared error\"\"\"\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Let's try a random guess: m=1, b=0\n",
    "m_guess, b_guess = 1, 0\n",
    "y_pred = predict(X, m_guess, b_guess)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "\n",
    "print(f\"With m={m_guess}, b={b_guess}: MSE = {mse:.2f}\")\n",
    "\n",
    "# Visualize how bad this guess is\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.7, label='Data')\n",
    "plt.plot([0, 10], [predict(0, m_guess, b_guess), predict(10, m_guess, b_guess)], \n",
    "         'r-', linewidth=2, label=f'Guess: y = {m_guess}x + {b_guess}')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title(f'Our First Guess (MSE = {mse:.2f})')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Finding the Best Line: The Closed-Form Solution\n",
    "\n",
    "There's actually a formula that gives us the optimal m and b directly!\n",
    "\n",
    "$$m = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}$$\n",
    "\n",
    "$$b = \\bar{y} - m\\bar{x}$$\n",
    "\n",
    "Where $\\bar{x}$ and $\\bar{y}$ are the means of X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear_regression(X, y):\n",
    "    \"\"\"\n",
    "    Find the optimal slope and intercept using the closed-form solution.\n",
    "    \n",
    "    This is the math you need to understand:\n",
    "    - We're finding m and b that minimize the sum of squared errors\n",
    "    - Taking derivatives and setting to zero gives us these formulas\n",
    "    \"\"\"\n",
    "    x_mean = np.mean(X)\n",
    "    y_mean = np.mean(y)\n",
    "    \n",
    "    # Numerator: sum of (x - x_mean) * (y - y_mean)\n",
    "    numerator = np.sum((X - x_mean) * (y - y_mean))\n",
    "    \n",
    "    # Denominator: sum of (x - x_mean)^2\n",
    "    denominator = np.sum((X - x_mean) ** 2)\n",
    "    \n",
    "    # Calculate slope and intercept\n",
    "    m = numerator / denominator\n",
    "    b = y_mean - m * x_mean\n",
    "    \n",
    "    return m, b\n",
    "\n",
    "# Fit our model\n",
    "m_optimal, b_optimal = fit_linear_regression(X, y)\n",
    "print(f\"Optimal parameters: m = {m_optimal:.4f}, b = {b_optimal:.4f}\")\n",
    "print(f\"True parameters:    m = {true_slope}, b = {true_intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how well our optimal line fits\n",
    "y_pred_optimal = predict(X, m_optimal, b_optimal)\n",
    "mse_optimal = mean_squared_error(y, y_pred_optimal)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.7, label='Data')\n",
    "plt.plot([0, 10], [predict(0, m_optimal, b_optimal), predict(10, m_optimal, b_optimal)], \n",
    "         'g-', linewidth=2, label=f'Fitted: y = {m_optimal:.2f}x + {b_optimal:.2f}')\n",
    "plt.plot([0, 10], [predict(0, true_slope, true_intercept), predict(10, true_slope, true_intercept)], \n",
    "         'r--', linewidth=2, alpha=0.5, label=f'True: y = {true_slope}x + {true_intercept}')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title(f'Fitted Line (MSE = {mse_optimal:.2f})')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Our MSE: {mse_optimal:.4f}\")\n",
    "print(f\"This is close to the variance of our noise ({2**2} = 4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing the Error Surface\n",
    "\n",
    "Let's see what the MSE looks like for different values of m and b. This helps us understand why our solution is optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of m and b values\n",
    "m_range = np.linspace(0, 4, 100)\n",
    "b_range = np.linspace(-3, 5, 100)\n",
    "M, B = np.meshgrid(m_range, b_range)\n",
    "\n",
    "# Calculate MSE for each combination\n",
    "MSE_grid = np.zeros_like(M)\n",
    "for i in range(M.shape[0]):\n",
    "    for j in range(M.shape[1]):\n",
    "        y_pred = predict(X, M[i,j], B[i,j])\n",
    "        MSE_grid[i,j] = mean_squared_error(y, y_pred)\n",
    "\n",
    "# Plot as a contour\n",
    "plt.figure(figsize=(10, 8))\n",
    "contour = plt.contour(M, B, MSE_grid, levels=20)\n",
    "plt.clabel(contour, inline=True, fontsize=8)\n",
    "plt.plot(m_optimal, b_optimal, 'r*', markersize=15, label='Our solution')\n",
    "plt.plot(true_slope, true_intercept, 'g^', markersize=12, label='True values')\n",
    "plt.xlabel('Slope (m)')\n",
    "plt.ylabel('Intercept (b)')\n",
    "plt.title('Error Surface: MSE for different m and b values')\n",
    "plt.legend()\n",
    "plt.colorbar(contour, label='MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The Matrix Form (Optional but Powerful)\n",
    "\n",
    "The same solution can be written with matrices. This generalizes to multiple variables:\n",
    "\n",
    "$$\\theta = (X^TX)^{-1}X^Ty$$\n",
    "\n",
    "Where $\\theta = [b, m]$ and X has a column of 1s for the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear_regression_matrix(X, y):\n",
    "    \"\"\"\n",
    "    Fit using the normal equation: theta = (X'X)^(-1) X'y\n",
    "    \n",
    "    This is exactly equivalent to our earlier solution,\n",
    "    but works for multiple features.\n",
    "    \"\"\"\n",
    "    # Add column of 1s for intercept\n",
    "    X_with_bias = np.column_stack([np.ones(len(X)), X])\n",
    "    \n",
    "    # Normal equation\n",
    "    theta = np.linalg.inv(X_with_bias.T @ X_with_bias) @ X_with_bias.T @ y\n",
    "    \n",
    "    b, m = theta[0], theta[1]\n",
    "    return m, b\n",
    "\n",
    "m_matrix, b_matrix = fit_linear_regression_matrix(X, y)\n",
    "print(f\"Matrix solution: m = {m_matrix:.4f}, b = {b_matrix:.4f}\")\n",
    "print(f\"Our solution:    m = {m_optimal:.4f}, b = {b_optimal:.4f}\")\n",
    "print(\"\\nThey match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Linear regression finds the best line** by minimizing the sum of squared errors\n",
    "2. **MSE is our loss function** - it tells us how wrong our predictions are\n",
    "3. **The closed-form solution** gives us the optimal m and b directly (no iteration needed)\n",
    "4. **The matrix form** generalizes to multiple variables\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "In the next notebook, we'll learn about **gradient descent** - an iterative method to find the minimum. This is more important because:\n",
    "- It works when we can't compute the closed-form solution\n",
    "- It's how neural networks learn\n",
    "- It scales better to huge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick summary\n",
    "print(\"=\" * 50)\n",
    "print(\"LINEAR REGRESSION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Data points: {len(X)}\")\n",
    "print(f\"True parameters: m={true_slope}, b={true_intercept}\")\n",
    "print(f\"Fitted parameters: m={m_optimal:.4f}, b={b_optimal:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse_optimal:.4f}\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
